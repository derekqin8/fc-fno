{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "299620ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60fcf2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import h5py\n",
    "import torch.nn as nn\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "\n",
    "#################################################\n",
    "#\n",
    "# Utilities\n",
    "#\n",
    "#################################################\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# reading data\n",
    "class MatReader(object):\n",
    "    def __init__(self, file_path, to_torch=True, to_cuda=False, to_float=True):\n",
    "        super(MatReader, self).__init__()\n",
    "\n",
    "        self.to_torch = to_torch\n",
    "        self.to_cuda = to_cuda\n",
    "        self.to_float = to_float\n",
    "\n",
    "        self.file_path = file_path\n",
    "\n",
    "        self.data = None\n",
    "        self.old_mat = None\n",
    "        self._load_file()\n",
    "\n",
    "    def _load_file(self):\n",
    "        self.data = scipy.io.loadmat(self.file_path)\n",
    "        self.old_mat = True\n",
    "        # try:\n",
    "        #     self.data = scipy.io.loadmat(self.file_path)\n",
    "        #     self.old_mat = True\n",
    "        # # except:\n",
    "        # #     self.data = h5py.File(self.file_path)\n",
    "        # #     self.old_mat = False\n",
    "\n",
    "    def load_file(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self._load_file()\n",
    "\n",
    "    def read_field(self, field):\n",
    "        x = self.data[field]\n",
    "\n",
    "        if not self.old_mat:\n",
    "            x = x[()]\n",
    "            x = np.transpose(x, axes=range(len(x.shape) - 1, -1, -1))\n",
    "\n",
    "        if self.to_float:\n",
    "            x = x.astype(np.float32)\n",
    "\n",
    "        if self.to_torch:\n",
    "            x = torch.from_numpy(x)\n",
    "\n",
    "            if self.to_cuda:\n",
    "                x = x.cuda()\n",
    "\n",
    "        return x\n",
    "\n",
    "    def set_cuda(self, to_cuda):\n",
    "        self.to_cuda = to_cuda\n",
    "\n",
    "    def set_torch(self, to_torch):\n",
    "        self.to_torch = to_torch\n",
    "\n",
    "    def set_float(self, to_float):\n",
    "        self.to_float = to_float\n",
    "\n",
    "# normalization, pointwise gaussian\n",
    "class UnitGaussianNormalizer(object):\n",
    "    def __init__(self, x, eps=0.00001):\n",
    "        super(UnitGaussianNormalizer, self).__init__()\n",
    "\n",
    "        # x could be in shape of ntrain*n or ntrain*T*n or ntrain*n*T\n",
    "        self.mean = torch.mean(x, 0)\n",
    "        self.std = torch.std(x, 0)\n",
    "        self.eps = eps\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = (x - self.mean) / (self.std + self.eps)\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, sample_idx=None):\n",
    "        if sample_idx is None:\n",
    "            std = self.std + self.eps # n\n",
    "            mean = self.mean\n",
    "        else:\n",
    "            if len(self.mean.shape) == len(sample_idx[0].shape):\n",
    "                std = self.std[sample_idx] + self.eps  # batch*n\n",
    "                mean = self.mean[sample_idx]\n",
    "            if len(self.mean.shape) > len(sample_idx[0].shape):\n",
    "                std = self.std[:,sample_idx]+ self.eps # T*batch*n\n",
    "                mean = self.mean[:,sample_idx]\n",
    "\n",
    "        # x is in shape of batch*n or T*batch*n\n",
    "        x = (x * std) + mean\n",
    "        return x\n",
    "\n",
    "    def cuda(self):\n",
    "        self.mean = self.mean.cuda()\n",
    "        self.std = self.std.cuda()\n",
    "\n",
    "    def cpu(self):\n",
    "        self.mean = self.mean.cpu()\n",
    "        self.std = self.std.cpu()\n",
    "\n",
    "# normalization, Gaussian\n",
    "class GaussianNormalizer(object):\n",
    "    def __init__(self, x, eps=0.00001):\n",
    "        super(GaussianNormalizer, self).__init__()\n",
    "\n",
    "        self.mean = torch.mean(x)\n",
    "        self.std = torch.std(x)\n",
    "        self.eps = eps\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = (x - self.mean) / (self.std + self.eps)\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, sample_idx=None):\n",
    "        x = (x * (self.std + self.eps)) + self.mean\n",
    "        return x\n",
    "\n",
    "    def cuda(self):\n",
    "        self.mean = self.mean.cuda()\n",
    "        self.std = self.std.cuda()\n",
    "\n",
    "    def cpu(self):\n",
    "        self.mean = self.mean.cpu()\n",
    "        self.std = self.std.cpu()\n",
    "\n",
    "\n",
    "# normalization, scaling by range\n",
    "class RangeNormalizer(object):\n",
    "    def __init__(self, x, low=0.0, high=1.0):\n",
    "        super(RangeNormalizer, self).__init__()\n",
    "        mymin = torch.min(x, 0)[0].view(-1)\n",
    "        mymax = torch.max(x, 0)[0].view(-1)\n",
    "\n",
    "        self.a = (high - low)/(mymax - mymin)\n",
    "        self.b = -self.a*mymax + high\n",
    "\n",
    "    def encode(self, x):\n",
    "        s = x.size()\n",
    "        x = x.view(s[0], -1)\n",
    "        x = self.a*x + self.b\n",
    "        x = x.view(s)\n",
    "        return x\n",
    "\n",
    "    def decode(self, x):\n",
    "        s = x.size()\n",
    "        x = x.view(s[0], -1)\n",
    "        x = (x - self.b)/self.a\n",
    "        x = x.view(s)\n",
    "        return x\n",
    "\n",
    "#loss function with rel/abs Lp loss\n",
    "class LpLoss(object):\n",
    "    def __init__(self, d=2, p=2, size_average=True, reduction=True):\n",
    "        super(LpLoss, self).__init__()\n",
    "\n",
    "        #Dimension and Lp-norm type are postive\n",
    "        assert d > 0 and p > 0\n",
    "\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.reduction = reduction\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def abs(self, x, y):\n",
    "        num_examples = x.size()[0]\n",
    "\n",
    "        #Assume uniform mesh\n",
    "        h = 1.0 / (x.size()[1] - 1.0)\n",
    "\n",
    "        all_norms = (h**(self.d/self.p))*torch.norm(x.view(num_examples,-1) - y.view(num_examples,-1), self.p, 1)\n",
    "\n",
    "        if self.reduction:\n",
    "            if self.size_average:\n",
    "                return torch.mean(all_norms)\n",
    "            else:\n",
    "                return torch.sum(all_norms)\n",
    "\n",
    "        return all_norms\n",
    "\n",
    "    def rel(self, x, y):\n",
    "        num_examples = x.size()[0]\n",
    "\n",
    "        diff_norms = torch.norm(x.reshape(num_examples,-1) - y.reshape(num_examples,-1), self.p, 1)\n",
    "        y_norms = torch.norm(y.reshape(num_examples,-1), self.p, 1)\n",
    "\n",
    "        if self.reduction:\n",
    "            if self.size_average:\n",
    "                return torch.mean(diff_norms/y_norms)\n",
    "            else:\n",
    "                return torch.sum(diff_norms/y_norms)\n",
    "\n",
    "        return diff_norms/y_norms\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        return self.rel(x, y)\n",
    "\n",
    "# Sobolev norm (HS norm)\n",
    "# where we also compare the numerical derivatives between the output and target\n",
    "class HsLoss(object):\n",
    "    def __init__(self, d=2, p=2, k=1, a=None, group=False, size_average=True, reduction=True):\n",
    "        super(HsLoss, self).__init__()\n",
    "\n",
    "        #Dimension and Lp-norm type are postive\n",
    "        assert d > 0 and p > 0\n",
    "\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.k = k\n",
    "        self.balanced = group\n",
    "        self.reduction = reduction\n",
    "        self.size_average = size_average\n",
    "\n",
    "        if a == None:\n",
    "            a = [1,] * k\n",
    "        self.a = a\n",
    "\n",
    "    def rel(self, x, y):\n",
    "        num_examples = x.size()[0]\n",
    "        diff_norms = torch.norm(x.reshape(num_examples,-1) - y.reshape(num_examples,-1), self.p, 1)\n",
    "        y_norms = torch.norm(y.reshape(num_examples,-1), self.p, 1)\n",
    "        if self.reduction:\n",
    "            if self.size_average:\n",
    "                return torch.mean(diff_norms/y_norms)\n",
    "            else:\n",
    "                return torch.sum(diff_norms/y_norms)\n",
    "        return diff_norms/y_norms\n",
    "\n",
    "    def __call__(self, x, y, a=None):\n",
    "        nx = x.size()[1]\n",
    "        ny = x.size()[2]\n",
    "        k = self.k\n",
    "        balanced = self.balanced\n",
    "        a = self.a\n",
    "        x = x.view(x.shape[0], nx, ny, -1)\n",
    "        y = y.view(y.shape[0], nx, ny, -1)\n",
    "\n",
    "        k_x = torch.cat((torch.arange(start=0, end=nx//2, step=1),torch.arange(start=-nx//2, end=0, step=1)), 0).reshape(nx,1).repeat(1,ny)\n",
    "        k_y = torch.cat((torch.arange(start=0, end=ny//2, step=1),torch.arange(start=-ny//2, end=0, step=1)), 0).reshape(1,ny).repeat(nx,1)\n",
    "        k_x = torch.abs(k_x).reshape(1,nx,ny,1).to(x.device)\n",
    "        k_y = torch.abs(k_y).reshape(1,nx,ny,1).to(x.device)\n",
    "\n",
    "        x = torch.fft.fftn(x, dim=[1, 2])\n",
    "        y = torch.fft.fftn(y, dim=[1, 2])\n",
    "\n",
    "        if balanced==False:\n",
    "            weight = 1\n",
    "            if k >= 1:\n",
    "                weight += a[0]**2 * (k_x**2 + k_y**2)\n",
    "            if k >= 2:\n",
    "                weight += a[1]**2 * (k_x**4 + 2*k_x**2*k_y**2 + k_y**4)\n",
    "            weight = torch.sqrt(weight)\n",
    "            loss = self.rel(x*weight, y*weight)\n",
    "        else:\n",
    "            loss = self.rel(x, y)\n",
    "            if k >= 1:\n",
    "                weight = a[0] * torch.sqrt(k_x**2 + k_y**2)\n",
    "                loss += self.rel(x*weight, y*weight)\n",
    "            if k >= 2:\n",
    "                weight = a[1] * torch.sqrt(k_x**4 + 2*k_x**2*k_y**2 + k_y**4)\n",
    "                loss += self.rel(x*weight, y*weight)\n",
    "            loss = loss / (k+1)\n",
    "\n",
    "        return loss\n",
    "\n",
    "# A simple feedforward neural network\n",
    "class DenseNet(torch.nn.Module):\n",
    "    def __init__(self, layers, nonlinearity, out_nonlinearity=None, normalize=False):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        self.n_layers = len(layers) - 1\n",
    "\n",
    "        assert self.n_layers >= 1\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for j in range(self.n_layers):\n",
    "            self.layers.append(nn.Linear(layers[j], layers[j+1]))\n",
    "\n",
    "            if j != self.n_layers - 1:\n",
    "                if normalize:\n",
    "                    self.layers.append(nn.BatchNorm1d(layers[j+1]))\n",
    "\n",
    "                self.layers.append(nonlinearity())\n",
    "\n",
    "        if out_nonlinearity is not None:\n",
    "            self.layers.append(out_nonlinearity())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for _, l in enumerate(self.layers):\n",
    "            x = l(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# print the number of parameters\n",
    "def count_params(model):\n",
    "    c = 0\n",
    "    for p in list(model.parameters()):\n",
    "        c += reduce(operator.mul, list(p.size()))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecbe451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compl_mul2d(a, b):\n",
    "    # (batch, in_channel, x,y,t ), (in_channel, out_channel, x,y,t) -> (batch, out_channel, x,y,t)\n",
    "    return torch.einsum(\"bixy,ioxy->boxy\", a, b)\n",
    "\n",
    "class LowRank2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(LowRank2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.phi = DenseNet([2, 64, 128, in_channels*out_channels], torch.nn.ReLU)\n",
    "        self.psi = DenseNet([2, 64, 128, in_channels*out_channels], torch.nn.ReLU)\n",
    "\n",
    "    def get_grid(self, S1, S2, batchsize, device):\n",
    "        gridx = torch.tensor(np.linspace(0, 1, S1), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, S1, 1).repeat([batchsize, 1, S2])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, S2), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, S2).repeat([batchsize, S1, 1])\n",
    "        return torch.stack((gridx, gridy), dim=-1).to(device)\n",
    "\n",
    "    def forward(self, x, gridy=None):\n",
    "        # x (batch, channel, x, y)\n",
    "        # y (Ny, 2)\n",
    "        batchsize, size1, size2 = x.shape[0], x.shape[2], x.shape[3]\n",
    "\n",
    "        gridx = self.get_grid(S1=size1, S2=size2, batchsize=1, device=x.device).reshape(size1 * size2, 2)\n",
    "        if gridy==None:\n",
    "            gridy = self.get_grid(S1=size1, S2=size2, batchsize=1, device=x.device).reshape(size1 * size2, 2)\n",
    "        Nx = size1 * size2\n",
    "        Ny = gridy.shape[0]\n",
    "\n",
    "        phi_eval = self.phi(gridx).reshape(Nx, self.out_channels, self.in_channels)\n",
    "        psi_eval = self.psi(gridy).reshape(Ny, self.out_channels, self.in_channels)\n",
    "        x = x.reshape(batchsize, self.in_channels, Nx)\n",
    "\n",
    "        x = torch.einsum('noi,bin,moi->bom', phi_eval, x, psi_eval) / Nx\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42fa9014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super(SpectralConv2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "\n",
    "    def forward(self, x, size=None):\n",
    "\n",
    "        if size==None:\n",
    "            size = (x.size(2), x.size(3))\n",
    "\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfftn(x, dim=[2,3])\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels, size[0], size[1]//2 + 1, device=x.device, dtype=torch.cfloat)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
    "            compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
    "            compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfftn(out_ft, s=(size[0], size[1]), dim=[2,3])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aac9b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBlock2d(nn.Module):\n",
    "    def __init__(self, modes1, modes2, width, in_dim=3, out_dim=1):\n",
    "        super(SimpleBlock2d, self).__init__()\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "\n",
    "        self.width_list = [width*2//4, width*3//4, width*4//4, width*4//4, width*5//4]\n",
    "\n",
    "        self.fc0 = nn.Linear(in_dim, self.width_list[0])\n",
    "\n",
    "        self.conv0 = SpectralConv2d(\n",
    "            self.width_list[0], self.width_list[1],\n",
    "            self.modes1*4//4, self.modes2*4//4\n",
    "        )\n",
    "        self.conv1 = SpectralConv2d(self.width_list[1], self.width_list[2], self.modes1*3//4, self.modes2*3//4)\n",
    "        self.conv2 = SpectralConv2d(self.width_list[2], self.width_list[3], self.modes1*2//4, self.modes2*2//4)\n",
    "        self.conv3 = SpectralConv2d(self.width_list[3], self.width_list[4], self.modes1*1//4, self.modes2*1//4)\n",
    "        self.w0 = nn.Conv1d(self.width_list[0], self.width_list[1], 1)\n",
    "        self.w1 = nn.Conv1d(self.width_list[1], self.width_list[2], 1)\n",
    "        self.w2 = nn.Conv1d(self.width_list[2], self.width_list[3], 1)\n",
    "        self.w3 = nn.Conv1d(self.width_list[3], self.width_list[4], 1)\n",
    "        self.k3 = LowRank2d(self.width_list[3], self.width_list[4])\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width_list[4], self.width_list[4]*2)\n",
    "        # self.fc2 = nn.Linear(self.width_list[4]*2, self.width_list[4]*2)\n",
    "        self.fc3 = nn.Linear(self.width_list[4]*2, out_dim)\n",
    "\n",
    "    def forward(self, x, sub=1):\n",
    "\n",
    "        batchsize = x.shape[0]\n",
    "        size_x, size_y = x.shape[1], x.shape[2]\n",
    "        size = (size_x*sub, size_y*sub)\n",
    "\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x.view(batchsize, self.width_list[0], size_x*size_y)).view(batchsize, self.width_list[1], size_x, size_y)\n",
    "        # x2 = F.interpolate(x2, size=size_list[1], mode='bilinear')\n",
    "        x = x1 + x2\n",
    "        x = F.elu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x.view(batchsize, self.width_list[1], size_x*size_y)).view(batchsize, self.width_list[2], size_x, size_y)\n",
    "        # x2 = F.interpolate(x2, size=size_list[2], mode='bilinear')\n",
    "        x = x1 + x2\n",
    "        x = F.elu(x)\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x.view(batchsize, self.width_list[2], size_x*size_y)).view(batchsize, self.width_list[3], size_x, size_y)\n",
    "        # x2 = F.interpolate(x2, size=size_list[3], mode='bilinear')\n",
    "        x = x1 + x2\n",
    "        x = F.elu(x)\n",
    "\n",
    "        x1 = self.conv3(x, size)\n",
    "        x2 = self.w3(x.view(batchsize, self.width_list[3], size_x*size_y)).view(batchsize, self.width_list[4], size_x, size_y)\n",
    "        # x2 = self.k3(x).reshape(batchsize, self.width_list[4], size_x, size_y)\n",
    "        # x2 = F.interpolate(x2, size=size, mode='bilinear')\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def get_grid(self, S, batchsize, device):\n",
    "        gridx = torch.tensor(np.linspace(0, 1, S), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, 1, S, 1).repeat([batchsize, 1, 1, S])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, S), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, 1, S).repeat([batchsize, 1, S, 1])\n",
    "\n",
    "        return torch.cat((gridx, gridy), dim=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4263a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2d(nn.Module):\n",
    "    def __init__(self, modes, width):\n",
    "        super(Net2d, self).__init__()\n",
    "\n",
    "        self.conv1 = SimpleBlock2d(modes, modes,  width)\n",
    "\n",
    "\n",
    "    def forward(self, x, sub=1):\n",
    "        x = self.conv1(x, sub)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def count_params(self):\n",
    "        c = 0\n",
    "        for p in self.parameters():\n",
    "            c += reduce(operator.mul, list(p.size()))\n",
    "\n",
    "        return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4101b8",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d9e2339",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 1024\n",
    "ntest = 2048 - ntrain\n",
    "\n",
    "sample_rate = 8\n",
    "data_points = 1024 // sample_rate\n",
    "\n",
    "sample_rate_t = 1\n",
    "time_points = 101 // sample_rate_t\n",
    "\n",
    "max_pad = 100\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "epochs = 50\n",
    "step_size = 10\n",
    "gamma = 0.5\n",
    "\n",
    "modes = 20\n",
    "width = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b027f7",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba3d416c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/scipy/io/matlab/mio.py:226: MatReadWarning: Duplicate variable name \"None\" in stream - replacing previous with new\n",
      "Consider mio5.varmats_from_mat to split file into single variable files\n",
      "  matfile_dict = MR.get_variables(variable_names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 128])\n",
      "torch.Size([2048, 101, 128])\n",
      "torch.Size([1024, 128])\n",
      "torch.Size([1024, 101, 128])\n",
      "torch.Size([1024, 128])\n",
      "torch.Size([1024, 101, 128])\n"
     ]
    }
   ],
   "source": [
    "data_loader = MatReader('burgers_v100_t100_r1024_N2048.mat')\n",
    "\n",
    "x_data_raw = data_loader.read_field('input')[:,::sample_rate]\n",
    "y_data_raw = data_loader.read_field('output')[:,::sample_rate_t,::sample_rate]\n",
    "\n",
    "print(x_data_raw.shape)\n",
    "print(y_data_raw.shape)\n",
    "\n",
    "x_train = x_data_raw[:ntrain]\n",
    "y_train = y_data_raw[:ntrain]\n",
    "\n",
    "x_test = x_data_raw[ntrain:]\n",
    "y_test = y_data_raw[ntrain:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e98e01a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_normalizer = UnitGaussianNormalizer(x_train)\n",
    "x_train = x_normalizer.encode(x_train)\n",
    "x_test = x_normalizer.encode(x_test)\n",
    "\n",
    "y_normalizer = UnitGaussianNormalizer(y_train)\n",
    "y_train = y_normalizer.encode(y_train)\n",
    "y_test = y_normalizer.encode(y_test)\n",
    "\n",
    "# Method from pino_burger_fdm\n",
    "# Repeats the 1D input to make it 2D\n",
    "x_train = x_train.reshape(ntrain, 1, data_points).repeat([1,time_points,1])\n",
    "x_test = x_test.reshape(ntest, 1, data_points).repeat([1,time_points,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b104b238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(x_train, x_test, pad_size):\n",
    "    # Padded data\n",
    "    padt_train = torch.zeros((ntrain, pad_size, data_points))\n",
    "    x_train_pad = torch.cat([x_train, padt_train], 1)\n",
    "\n",
    "    padt_test = torch.zeros((ntest, pad_size, data_points))\n",
    "    x_test_pad = torch.cat([x_test, padt_test], 1)\n",
    "\n",
    "    # Adding grids\n",
    "    gridt_pad = torch.tensor(np.linspace(0,1,time_points+pad_size), dtype = torch.float)\n",
    "    gridt_pad = gridt_pad.reshape(1,time_points+pad_size,1)\n",
    "    gridx_pad = torch.tensor(np.linspace(0,1,data_points), dtype = torch.float)\n",
    "    gridx_pad = gridx_pad.reshape(1,1,data_points)\n",
    "\n",
    "    x_train_pad_grids = torch.stack([x_train_pad, gridx_pad.repeat([ntrain, time_points+pad_size, 1]), gridt_pad.repeat([ntrain, 1, data_points])], dim = 3)\n",
    "    x_test_pad_grids = torch.stack([x_test_pad, gridx_pad.repeat([ntest, time_points+pad_size, 1]), gridt_pad.repeat([ntest, 1, data_points])], dim = 3)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train_pad_grids, y_train), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test_pad_grids, y_test), batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "677b709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, scheduler, l2_loss, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "        \n",
    "        # remove padding\n",
    "        output = output.narrow(1, 0, time_points)\n",
    "        \n",
    "        # loss = F.mse_loss(output.view(batch_size,-1), target.view(batch_size,-1), reduction = 'sum')\n",
    "        loss = l2_loss(output.view(batch_size, time_points, data_points), target.view(batch_size, time_points, data_points))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    return total_loss / ntrain\n",
    "\n",
    "def test(test_loader, model, l2_loss):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            \n",
    "            # remove padding\n",
    "            output = output.narrow(1, 0, time_points)\n",
    "            \n",
    "            # loss += F.mse_loss(output.view(batch_size, -1), target.view(batch_size,-1), reduction = 'sum').item()\n",
    "            loss += l2_loss(output.view(batch_size, time_points, data_points), target.view(batch_size, time_points, data_points)).item()\n",
    "    \n",
    "    return loss / ntest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bac0ce82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad: 0\n",
      "Run: 0\n",
      "1.022838952136226\n",
      "Epoch: 0\n",
      "0.15914950535079697\n",
      "0.09926419097610051\n",
      "Epoch: 1\n",
      "0.08550312575971475\n",
      "0.0776869786423049\n",
      "Epoch: 2\n",
      "0.0617654387751827\n",
      "0.04428435219961102\n",
      "Epoch: 3\n",
      "0.053834937565625296\n",
      "0.06365839987483923\n",
      "Epoch: 4\n",
      "0.05049482832873764\n",
      "0.04608601022118819\n",
      "Epoch: 5\n",
      "0.053790268353623105\n",
      "0.058418576190888416\n",
      "Epoch: 6\n",
      "0.04436421317404893\n",
      "0.03467795286087494\n",
      "Epoch: 7\n",
      "0.0440931094744883\n",
      "0.03314783911082486\n",
      "Epoch: 8\n",
      "0.04458990995044587\n",
      "0.03658118290150014\n",
      "Epoch: 9\n",
      "0.04226070978347707\n",
      "0.04110150213455199\n",
      "Epoch: 10\n",
      "0.02504791203318746\n",
      "0.03398922752330691\n",
      "Epoch: 11\n",
      "0.02306387323369563\n",
      "0.022700975462612405\n",
      "Epoch: 12\n",
      "0.022873640141369833\n",
      "0.024920169802499004\n",
      "Epoch: 13\n",
      "0.024346299629542045\n",
      "0.01949787152261706\n",
      "Epoch: 14\n",
      "0.024568482163886074\n",
      "0.020749281682583387\n",
      "Epoch: 15\n",
      "0.02353064698036178\n",
      "0.01976280681174103\n",
      "Epoch: 16\n",
      "0.022995535185145854\n",
      "0.02247181330312742\n",
      "Epoch: 17\n",
      "0.02160644233754283\n",
      "0.02208847347264964\n",
      "Epoch: 18\n",
      "0.022210851958334388\n",
      "0.023353278776994557\n",
      "Epoch: 19\n",
      "0.024010194867514656\n",
      "0.02303610957824276\n",
      "Epoch: 20\n",
      "0.015417980564052414\n",
      "0.013676148377271602\n",
      "Epoch: 21\n",
      "0.015606637237851828\n",
      "0.01278267616089579\n",
      "Epoch: 22\n",
      "0.015523492600095778\n",
      "0.015735896369733382\n",
      "Epoch: 23\n",
      "0.014208268254606082\n",
      "0.018248000964376843\n",
      "Epoch: 24\n",
      "0.015671949131956353\n",
      "0.012789937096840731\n",
      "Epoch: 25\n",
      "0.015382343232431595\n",
      "0.018638990916770126\n",
      "Epoch: 26\n",
      "0.015185699308403855\n",
      "0.013094780892970448\n",
      "Epoch: 27\n",
      "0.015936027093175653\n",
      "0.012422483728187217\n",
      "Epoch: 28\n",
      "0.015316030915528245\n",
      "0.01321081242940636\n",
      "Epoch: 29\n",
      "0.014887332348735072\n",
      "0.014583511880573496\n",
      "Epoch: 30\n",
      "0.011205093536773347\n",
      "0.012289616627640498\n",
      "Epoch: 31\n",
      "0.011290800421193126\n",
      "0.011225865262076695\n",
      "Epoch: 32\n",
      "0.01112376523860803\n",
      "0.010765821869426873\n",
      "Epoch: 33\n",
      "0.011148455446800654\n",
      "0.01090599687950089\n",
      "Epoch: 34\n",
      "0.011020757568530826\n",
      "0.011008716229298443\n",
      "Epoch: 35\n",
      "0.011014793907634157\n",
      "0.00934092386660268\n",
      "Epoch: 36\n",
      "0.010846881395536911\n",
      "0.010681119234959624\n",
      "Epoch: 37\n",
      "0.01060633937458988\n",
      "0.009589259069343825\n",
      "Epoch: 38\n",
      "0.010441730435104546\n",
      "0.01225824925222696\n",
      "Epoch: 39\n",
      "0.01057010000158698\n",
      "0.01057872583533026\n",
      "Epoch: 40\n",
      "0.00887572241936141\n",
      "0.009195760107559181\n",
      "Epoch: 41\n",
      "0.008814514994355704\n",
      "0.008545561470782559\n",
      "Epoch: 42\n",
      "0.008741774572627037\n",
      "0.008976803336281591\n",
      "Epoch: 43\n",
      "0.008708248089988047\n",
      "0.008773143788857851\n",
      "Epoch: 44\n",
      "0.00863768008957777\n",
      "0.010720079272232397\n",
      "Epoch: 45\n",
      "0.00880140651770489\n",
      "0.008301698534069146\n",
      "Epoch: 46\n",
      "0.008426225710536528\n",
      "0.008345008779087948\n",
      "Epoch: 47\n",
      "0.008553625154945621\n",
      "0.00806895157074905\n",
      "Epoch: 48\n",
      "0.008433316979790106\n",
      "0.008989359036149835\n",
      "Epoch: 49\n",
      "0.00851030973262823\n",
      "0.008677047230776225\n",
      "0 0.008677047230776225\n",
      "Run: 1\n",
      "1.0097449460299686\n",
      "Epoch: 0\n",
      "0.1625754589731514\n",
      "0.08196786126063671\n",
      "Epoch: 1\n",
      "0.0810698342284013\n",
      "0.05406099277024623\n",
      "Epoch: 2\n",
      "0.06655106794642052\n",
      "0.06123977171773731\n",
      "Epoch: 3\n",
      "0.05465924530471966\n",
      "0.06469153179932619\n",
      "Epoch: 4\n",
      "0.0489116603621369\n",
      "0.050200103572933585\n",
      "Epoch: 5\n",
      "0.045250583409142564\n",
      "0.036899783142871456\n",
      "Epoch: 6\n",
      "0.04274833332783601\n",
      "0.04151395204644359\n",
      "Epoch: 7\n",
      "0.04368447744855075\n",
      "0.06230323799900361\n",
      "Epoch: 8\n",
      "0.04538392574067984\n",
      "0.04348003467566741\n",
      "Epoch: 9\n",
      "0.039751688644173555\n",
      "0.0341810169229575\n",
      "Epoch: 10\n",
      "0.024108648613037076\n",
      "0.01954527486304869\n",
      "Epoch: 11\n",
      "0.022936293316888623\n",
      "0.018560544401225343\n",
      "Epoch: 12\n",
      "0.023554855391012097\n",
      "0.02348141140282678\n",
      "Epoch: 13\n",
      "0.024326060133716965\n",
      "0.021194006296354928\n",
      "Epoch: 14\n",
      "0.023168756218183262\n",
      "0.020674747785051295\n",
      "Epoch: 15\n",
      "0.0236549994433517\n",
      "0.017835364827078592\n",
      "Epoch: 16\n",
      "0.021439715094857092\n",
      "0.025354228696414793\n",
      "Epoch: 17\n",
      "0.02137557839978399\n",
      "0.026695734556597017\n",
      "Epoch: 18\n",
      "0.023004415400464495\n",
      "0.02236357517358556\n",
      "Epoch: 19\n",
      "0.023093224165677384\n",
      "0.028480671253419132\n",
      "Epoch: 20\n",
      "0.015666699497160153\n",
      "0.015305567542782228\n",
      "Epoch: 21\n",
      "0.014886075927734055\n",
      "0.01551767038199614\n",
      "Epoch: 22\n",
      "0.014702149147069576\n",
      "0.013917984432737285\n",
      "Epoch: 23\n",
      "0.014813119711106992\n",
      "0.014433191441185045\n",
      "Epoch: 24\n",
      "0.01459254280098321\n",
      "0.012952433437021682\n",
      "Epoch: 25\n",
      "0.015259012787737447\n",
      "0.014533736758494342\n",
      "Epoch: 26\n",
      "0.014159242708501552\n",
      "0.01749898530124483\n",
      "Epoch: 27\n",
      "0.014482071356269444\n",
      "0.015373627035842219\n",
      "Epoch: 28\n",
      "0.015192179177120124\n",
      "0.013855096347469953\n",
      "Epoch: 29\n",
      "0.014880002757763577\n",
      "0.01635332284240576\n",
      "Epoch: 30\n",
      "0.010603281793464703\n",
      "0.010881103899009759\n",
      "Epoch: 31\n",
      "0.011037296240829164\n",
      "0.011204759646716411\n",
      "Epoch: 32\n",
      "0.010988825917138456\n",
      "0.012110878644762124\n",
      "Epoch: 33\n",
      "0.010675324126168562\n",
      "0.009733711440276238\n",
      "Epoch: 34\n",
      "0.010711376249219029\n",
      "0.010916971300048317\n",
      "Epoch: 35\n",
      "0.01061939210012497\n",
      "0.01048284998478266\n",
      "Epoch: 36\n",
      "0.010779256735531817\n",
      "0.009655903001657862\n",
      "Epoch: 37\n",
      "0.010657645864739607\n",
      "0.01164091787813959\n",
      "Epoch: 38\n",
      "0.010628275729231973\n",
      "0.012665894066685723\n",
      "Epoch: 39\n",
      "0.010569572430995322\n",
      "0.0093892429126754\n",
      "Epoch: 40\n",
      "0.00865338117569081\n",
      "0.00863691622407714\n",
      "Epoch: 41\n",
      "0.008898236401819304\n",
      "0.008637036939489917\n",
      "Epoch: 42\n",
      "0.008696155730376631\n",
      "0.008670188347423391\n",
      "Epoch: 43\n",
      "0.008561008974993456\n",
      "0.008947826512667234\n",
      "Epoch: 44\n",
      "0.008662012396598584\n",
      "0.010511712138395524\n",
      "Epoch: 45\n",
      "0.008604990919593547\n",
      "0.00855266111648234\n",
      "Epoch: 46\n",
      "0.008530500051620038\n",
      "0.008272277663763816\n",
      "Epoch: 47\n",
      "0.008521058995938802\n",
      "0.008846253878346033\n",
      "Epoch: 48\n",
      "0.008527254475211521\n",
      "0.0081251984975097\n",
      "Epoch: 49\n",
      "0.008325081018938363\n",
      "0.00830617704332326\n",
      "1 0.00830617704332326\n",
      "Run: 2\n",
      "1.009133951854892\n",
      "Epoch: 0\n",
      "0.1825182741667959\n",
      "0.08250969538858044\n",
      "Epoch: 1\n",
      "0.08286481660616118\n",
      "0.07239565305280848\n",
      "Epoch: 2\n",
      "0.06176136682552169\n",
      "0.04909955862967763\n",
      "Epoch: 3\n",
      "0.0542521736715571\n",
      "0.05417873965961917\n",
      "Epoch: 4\n",
      "0.050450596143491566\n",
      "0.04579903588637535\n",
      "Epoch: 5\n",
      "0.04457797374379879\n",
      "0.03988014934111561\n",
      "Epoch: 6\n",
      "0.04719728630152531\n",
      "0.051303739155628136\n",
      "Epoch: 7\n",
      "0.044677159966340696\n",
      "0.036527407964968006\n",
      "Epoch: 8\n",
      "0.04100343168829568\n",
      "0.04659188002369774\n",
      "Epoch: 9\n",
      "0.04282842109751073\n",
      "0.04166435067054408\n",
      "Epoch: 10\n",
      "0.02435316118862829\n",
      "0.026058908881168463\n",
      "Epoch: 11\n",
      "0.024378504364904074\n",
      "0.020345712389826076\n",
      "Epoch: 12\n",
      "0.02337878764319612\n",
      "0.021460609254063456\n",
      "Epoch: 13\n",
      "0.023755037467708462\n",
      "0.028100339199227164\n",
      "Epoch: 14\n",
      "0.023554449081530038\n",
      "0.02000995564412733\n",
      "Epoch: 15\n",
      "0.021927465984845185\n",
      "0.025303763162810355\n",
      "Epoch: 16\n",
      "0.021733042533924163\n",
      "0.01864564197603613\n",
      "Epoch: 17\n",
      "0.02275623482728406\n",
      "0.022097694317380956\n",
      "Epoch: 18\n",
      "0.022080737316173327\n",
      "0.026778248357004486\n",
      "Epoch: 19\n",
      "0.02082360272288497\n",
      "0.020923281311297615\n",
      "Epoch: 20\n",
      "0.014842281128039758\n",
      "0.01416405778581975\n",
      "Epoch: 21\n",
      "0.015133854551095283\n",
      "0.016435739586995624\n",
      "Epoch: 22\n",
      "0.014766928741209995\n",
      "0.013881372351079335\n",
      "Epoch: 23\n",
      "0.014997035891610722\n",
      "0.012746614571369719\n",
      "Epoch: 24\n",
      "0.015165220219842013\n",
      "0.01294732701171597\n",
      "Epoch: 25\n",
      "0.014523966302476765\n",
      "0.01398204085035104\n",
      "Epoch: 26\n",
      "0.015153902674683195\n",
      "0.01908113677836809\n",
      "Epoch: 27\n",
      "0.014810968840720307\n",
      "0.013945150627478142\n",
      "Epoch: 28\n",
      "0.014482386485724419\n",
      "0.024201655322031\n",
      "Epoch: 29\n",
      "0.01507745080289169\n",
      "0.01355556724638518\n",
      "Epoch: 30\n",
      "0.011050968226300029\n",
      "0.011209434748252534\n",
      "Epoch: 31\n",
      "0.011076455210513814\n",
      "0.010647599598087254\n",
      "Epoch: 32\n",
      "0.010967870872718777\n",
      "0.010827585477727553\n",
      "Epoch: 33\n",
      "0.010870015671116562\n",
      "0.011253488492457109\n",
      "Epoch: 34\n",
      "0.010462757862114813\n",
      "0.01334685976326\n",
      "Epoch: 35\n",
      "0.010660957961590611\n",
      "0.010227738528101327\n",
      "Epoch: 36\n",
      "0.010720419965309702\n",
      "0.011200836396255909\n",
      "Epoch: 37\n",
      "0.010783304131109617\n",
      "0.01010006262913521\n",
      "Epoch: 38\n",
      "0.01047406872476131\n",
      "0.010411625083179388\n",
      "Epoch: 39\n",
      "0.010740822374373238\n",
      "0.010260803477194713\n",
      "Epoch: 40\n",
      "0.008778136718319729\n",
      "0.008906826960810577\n",
      "Epoch: 41\n",
      "0.008679773341100372\n",
      "0.008377173885946831\n",
      "Epoch: 42\n",
      "0.008730189264497312\n",
      "0.008951061467541876\n",
      "Epoch: 43\n",
      "0.008731813975828118\n",
      "0.009698436961571133\n",
      "Epoch: 44\n",
      "0.008608822583482834\n",
      "0.008254749307525344\n",
      "Epoch: 45\n",
      "0.008522085690401582\n",
      "0.00809875963250306\n",
      "Epoch: 46\n",
      "0.008491092253734678\n",
      "0.008802722205018654\n",
      "Epoch: 47\n",
      "0.008535717064205528\n",
      "0.008620843177595816\n",
      "Epoch: 48\n",
      "0.008409564609792142\n",
      "0.008009279452835472\n",
      "Epoch: 49\n",
      "0.008341734143868962\n",
      "0.008819241140827216\n",
      "2 0.008819241140827216\n",
      "Run: 3\n",
      "1.00282542349305\n",
      "Epoch: 0\n",
      "0.16714875739853596\n",
      "0.07388872120645829\n",
      "Epoch: 1\n",
      "0.08640988792831195\n",
      "0.08313602699854528\n",
      "Epoch: 2\n",
      "0.06674842031497974\n",
      "0.06878005319231306\n",
      "Epoch: 3\n",
      "0.05903242243039131\n",
      "0.04387431969735189\n",
      "Epoch: 4\n",
      "0.04804098836211779\n",
      "0.07924320183155942\n",
      "Epoch: 5\n",
      "0.057439559619524516\n",
      "0.048920919516604044\n",
      "Epoch: 6\n",
      "0.043008690498027136\n",
      "0.03434592469238851\n",
      "Epoch: 7\n",
      "0.0435583977123315\n",
      "0.07122819922369672\n",
      "Epoch: 8\n",
      "0.04359537817981618\n",
      "0.03894168237093254\n",
      "Epoch: 9\n",
      "0.03890229937132972\n",
      "0.04309826330791111\n",
      "Epoch: 10\n",
      "0.024755859861215868\n",
      "0.0386367458222594\n",
      "Epoch: 11\n",
      "0.023665755578804237\n",
      "0.0200010021799244\n",
      "Epoch: 12\n",
      "0.02457756618332496\n",
      "0.021808047248669027\n",
      "Epoch: 13\n",
      "0.024046716988777916\n",
      "0.025293283316386805\n",
      "Epoch: 14\n",
      "0.02248562253498676\n",
      "0.019285077262793493\n",
      "Epoch: 15\n",
      "0.02286842164267\n",
      "0.019329876912706823\n",
      "Epoch: 16\n",
      "0.022196872730091854\n",
      "0.01775490272029856\n",
      "Epoch: 17\n",
      "0.022963040275499225\n",
      "0.01929062024828454\n",
      "Epoch: 18\n",
      "0.023093953592251637\n",
      "0.02362298142088548\n",
      "Epoch: 19\n",
      "0.023003527393484546\n",
      "0.020242533141754393\n",
      "Epoch: 20\n",
      "0.015561478154268116\n",
      "0.01770766457229911\n",
      "Epoch: 21\n",
      "0.015439867368058913\n",
      "0.014068297030462418\n",
      "Epoch: 22\n",
      "0.014578768551018584\n",
      "0.016688588433680707\n",
      "Epoch: 23\n",
      "0.015297637475214287\n",
      "0.014790320212341612\n",
      "Epoch: 24\n",
      "0.014783260195144976\n",
      "0.012259214445748512\n",
      "Epoch: 25\n",
      "0.014623129721712758\n",
      "0.013645284841004468\n",
      "Epoch: 26\n",
      "0.015003337154212204\n",
      "0.01672830498773692\n",
      "Epoch: 27\n",
      "0.015033412409593438\n",
      "0.01621277151571121\n",
      "Epoch: 28\n",
      "0.015085773411556147\n",
      "0.018223588741420826\n",
      "Epoch: 29\n",
      "0.014831977123776596\n",
      "0.017663713245383406\n",
      "Epoch: 30\n",
      "0.01123482952652921\n",
      "0.011216099208468222\n",
      "Epoch: 31\n",
      "0.011105281669642864\n",
      "0.014324838571155851\n",
      "Epoch: 32\n",
      "0.010924993403477856\n",
      "0.012424331700458424\n",
      "Epoch: 33\n",
      "0.01084651987775942\n",
      "0.010566421322437236\n",
      "Epoch: 34\n",
      "0.011213906047487399\n",
      "0.010742658217623102\n",
      "Epoch: 35\n",
      "0.010602449436646566\n",
      "0.010395979339591577\n",
      "Epoch: 36\n",
      "0.010739926420228585\n",
      "0.010926705444035179\n",
      "Epoch: 37\n",
      "0.010850828797174472\n",
      "0.010131636893220275\n",
      "Epoch: 38\n",
      "0.01050351005005723\n",
      "0.010158744571981515\n",
      "Epoch: 39\n",
      "0.010815967517373792\n",
      "0.01140805850036486\n",
      "Epoch: 40\n",
      "0.00909969029316926\n",
      "0.008505324815359927\n",
      "Epoch: 41\n",
      "0.008990896031264128\n",
      "0.009152202109362406\n",
      "Epoch: 42\n",
      "0.008927409483931115\n",
      "0.008963795099134586\n",
      "Epoch: 43\n",
      "0.008870266195572185\n",
      "0.009369622180201986\n",
      "Epoch: 44\n",
      "0.008857570317559293\n",
      "0.008715417620805965\n",
      "Epoch: 45\n",
      "0.008713943544535141\n",
      "0.008712403162007831\n",
      "Epoch: 46\n",
      "0.008722232448235445\n",
      "0.008486699693094124\n",
      "Epoch: 47\n",
      "0.008624256036455336\n",
      "0.008308125730309257\n",
      "Epoch: 48\n",
      "0.008668427153224911\n",
      "0.008949336405748909\n",
      "Epoch: 49\n",
      "0.008648956453725987\n",
      "0.008388054051465588\n",
      "3 0.008388054051465588\n",
      "Run: 4\n",
      "1.0294122755294666\n",
      "Epoch: 0\n",
      "0.17133232844571467\n",
      "0.1245631176934694\n",
      "Epoch: 1\n",
      "0.08672031003516167\n",
      "0.078300631041202\n",
      "Epoch: 2\n",
      "0.0647249799749261\n",
      "0.04988555501222436\n",
      "Epoch: 3\n",
      "0.05871273380034836\n",
      "0.05338356647735054\n",
      "Epoch: 4\n",
      "0.04917335590471339\n",
      "0.04093262242167839\n",
      "Epoch: 5\n",
      "0.0469496144396544\n",
      "0.04733238106200588\n",
      "Epoch: 6\n",
      "0.049107134520454565\n",
      "0.03996094196372724\n",
      "Epoch: 7\n",
      "0.04546866816235706\n",
      "0.04559779404007713\n",
      "Epoch: 8\n",
      "0.039996675267502724\n",
      "0.03925287700985791\n",
      "Epoch: 9\n",
      "0.040518836963201466\n",
      "0.05033874930268212\n",
      "Epoch: 10\n",
      "0.025205618460859114\n",
      "0.01995483473092463\n",
      "Epoch: 11\n",
      "0.025001833477290347\n",
      "0.022114838712695928\n",
      "Epoch: 12\n",
      "0.024201719604207028\n",
      "0.0202893764617329\n",
      "Epoch: 13\n",
      "0.022862037518279976\n",
      "0.021480805978171702\n",
      "Epoch: 14\n",
      "0.023757385500175587\n",
      "0.019423922769419733\n",
      "Epoch: 15\n",
      "0.023594959377987834\n",
      "0.020575141174958844\n",
      "Epoch: 16\n",
      "0.022729960802280402\n",
      "0.019507533668729593\n",
      "Epoch: 17\n",
      "0.02422811423002713\n",
      "0.019780481415182294\n",
      "Epoch: 18\n",
      "0.02364749970547564\n",
      "0.027245427963862312\n",
      "Epoch: 19\n",
      "0.023399427934236883\n",
      "0.024945035890596046\n",
      "Epoch: 20\n",
      "0.015241374587731116\n",
      "0.015183587729552528\n",
      "Epoch: 21\n",
      "0.015008126882548822\n",
      "0.020430993009540543\n",
      "Epoch: 22\n",
      "0.016019707165924046\n",
      "0.014364600668614003\n",
      "Epoch: 23\n",
      "0.015128556767194823\n",
      "0.022582313339626126\n",
      "Epoch: 24\n",
      "0.015390277341339242\n",
      "0.012781066608567926\n",
      "Epoch: 25\n",
      "0.014524712050842936\n",
      "0.014476093679149926\n",
      "Epoch: 26\n",
      "0.014901814345648745\n",
      "0.019972929084360658\n",
      "Epoch: 27\n",
      "0.014961366953684774\n",
      "0.01281872790059424\n",
      "Epoch: 28\n",
      "0.014624003943481512\n",
      "0.016958109248662367\n",
      "Epoch: 29\n",
      "0.015729451452443755\n",
      "0.016869439931724628\n",
      "Epoch: 30\n",
      "0.011058254187446437\n",
      "0.010507141121252062\n",
      "Epoch: 31\n",
      "0.010696344111238432\n",
      "0.010109046849720471\n",
      "Epoch: 32\n",
      "0.011073404343278526\n",
      "0.011609232047248952\n",
      "Epoch: 33\n",
      "0.011260638027579262\n",
      "0.010748379070719238\n",
      "Epoch: 34\n",
      "0.010576193795259314\n",
      "0.010441709076076222\n",
      "Epoch: 35\n",
      "0.010497286250938487\n",
      "0.01048892213293584\n",
      "Epoch: 36\n",
      "0.010966751065097924\n",
      "0.010254365573473478\n",
      "Epoch: 37\n",
      "0.010401672472653445\n",
      "0.010355590163726447\n",
      "Epoch: 38\n",
      "0.010702539254452859\n",
      "0.009427508853605104\n",
      "Epoch: 39\n",
      "0.010390550455213088\n",
      "0.011255340834850358\n",
      "Epoch: 40\n",
      "0.00849495729607952\n",
      "0.008757937822792883\n",
      "Epoch: 41\n",
      "0.008752212886520283\n",
      "0.008306920312406874\n",
      "Epoch: 42\n",
      "0.008625475681128592\n",
      "0.008091225314728945\n",
      "Epoch: 43\n",
      "0.008565181351059437\n",
      "0.008365316936760792\n",
      "Epoch: 44\n",
      "0.008394796775519353\n",
      "0.008268947835404106\n",
      "Epoch: 45\n",
      "0.008491993469760928\n",
      "0.008917804994780454\n",
      "Epoch: 46\n",
      "0.008483777366564027\n",
      "0.0080380213130411\n",
      "Epoch: 47\n",
      "0.008358668885648513\n",
      "0.008256773179709853\n",
      "Epoch: 48\n",
      "0.008401132696235436\n",
      "0.009180758756883733\n",
      "Epoch: 49\n",
      "0.008270010170690512\n",
      "0.008936090873248759\n",
      "4 0.008936090873248759\n",
      "Run: 5\n",
      "1.0309977980796248\n",
      "Epoch: 0\n",
      "0.1733029424212873\n",
      "0.12477077784569701\n",
      "Epoch: 1\n",
      "0.07987099975252931\n",
      "0.08304615867018583\n",
      "Epoch: 2\n",
      "0.06308946523131453\n",
      "0.050729481039525126\n",
      "Epoch: 3\n",
      "0.05215957206928579\n",
      "0.05596628193961806\n",
      "Epoch: 4\n",
      "0.050033051948048524\n",
      "0.03397246755230299\n",
      "Epoch: 5\n",
      "0.04751985712937312\n",
      "0.07256708762361086\n",
      "Epoch: 6\n",
      "0.04399526770612283\n",
      "0.03631080953164201\n",
      "Epoch: 7\n",
      "0.04796221315882576\n",
      "0.04928582302090945\n",
      "Epoch: 8\n",
      "0.042219073004162055\n",
      "0.038395325198507635\n",
      "Epoch: 9\n",
      "0.04331197293504374\n",
      "0.03423100743748364\n",
      "Epoch: 10\n",
      "0.025461222256126348\n",
      "0.03208715993514488\n",
      "Epoch: 11\n",
      "0.025064217405997624\n",
      "0.022804840567914653\n",
      "Epoch: 12\n",
      "0.023216612531541614\n",
      "0.02081520536739845\n",
      "Epoch: 13\n",
      "0.023177971889708715\n",
      "0.018531699218328868\n",
      "Epoch: 14\n",
      "0.023734731916192686\n",
      "0.022082907128606166\n",
      "Epoch: 15\n",
      "0.023070757969435363\n",
      "0.019333887778884673\n",
      "Epoch: 16\n",
      "0.024261689231934724\n",
      "0.01970292671740026\n",
      "Epoch: 17\n",
      "0.023357893712272926\n",
      "0.0284855976660765\n",
      "Epoch: 18\n",
      "0.02427588502996514\n",
      "0.02689577232376905\n",
      "Epoch: 19\n",
      "0.02308234355677996\n",
      "0.021567488004620827\n",
      "Epoch: 20\n",
      "0.014890038340581668\n",
      "0.013723948464757996\n",
      "Epoch: 21\n",
      "0.014852676230475481\n",
      "0.013315022918959585\n",
      "Epoch: 22\n",
      "0.014960239633182937\n",
      "0.01501739397917845\n",
      "Epoch: 23\n",
      "0.015058231002512912\n",
      "0.012489961437950114\n",
      "Epoch: 24\n",
      "0.015008708312052477\n",
      "0.01298441047219967\n",
      "Epoch: 25\n",
      "0.015347394189120678\n",
      "0.022390768063814903\n",
      "Epoch: 26\n",
      "0.014836660198398022\n",
      "0.017685060915937356\n",
      "Epoch: 27\n",
      "0.014812670249284565\n",
      "0.015160123676650983\n",
      "Epoch: 28\n",
      "0.014393601809388201\n",
      "0.01779800277290633\n",
      "Epoch: 29\n",
      "0.01532284393442751\n",
      "0.013863471493095858\n",
      "Epoch: 30\n",
      "0.01091760498502481\n",
      "0.010394566984814446\n",
      "Epoch: 31\n",
      "0.010932415184470301\n",
      "0.012458123478154448\n",
      "Epoch: 32\n",
      "0.011050592747324117\n",
      "0.011629062290467118\n",
      "Epoch: 33\n",
      "0.010600878776585887\n",
      "0.009556001144119364\n",
      "Epoch: 34\n",
      "0.010914892189703096\n",
      "0.010567153758984205\n",
      "Epoch: 35\n",
      "0.010787565997816273\n",
      "0.010707314663250145\n",
      "Epoch: 36\n",
      "0.01085716536590553\n",
      "0.010234553573354788\n",
      "Epoch: 37\n",
      "0.010448923739204474\n",
      "0.009761286838966043\n",
      "Epoch: 38\n",
      "0.010742223015313357\n",
      "0.009685363866537955\n",
      "Epoch: 39\n",
      "0.010869616368381685\n",
      "0.01531362001742309\n",
      "Epoch: 40\n",
      "0.00885427809544126\n",
      "0.008658748164179997\n",
      "Epoch: 41\n",
      "0.008838487222419644\n",
      "0.008683239253969077\n",
      "Epoch: 42\n",
      "0.008816375868264004\n",
      "0.008528789963293093\n",
      "Epoch: 43\n",
      "0.008769773836320383\n",
      "0.008430844166923634\n",
      "Epoch: 44\n",
      "0.008604786456999136\n",
      "0.008548778122076328\n",
      "Epoch: 45\n",
      "0.008526319125849113\n",
      "0.008955006014275568\n",
      "Epoch: 46\n",
      "0.00870094269475885\n",
      "0.0091340197250247\n",
      "Epoch: 47\n",
      "0.008558552348404191\n",
      "0.00888122936430591\n",
      "Epoch: 48\n",
      "0.008654570878661616\n",
      "0.008262847342393798\n",
      "Epoch: 49\n",
      "0.008597150747391424\n",
      "0.008297824384953856\n",
      "5 0.008297824384953856\n",
      "Run: 6\n",
      "1.01727147877682\n",
      "Epoch: 0\n",
      "0.16126814713061322\n",
      "0.09959256484580692\n",
      "Epoch: 1\n",
      "0.08299790263663454\n",
      "0.06020707056632091\n",
      "Epoch: 2\n",
      "0.07012329268400208\n",
      "0.06208957232956891\n",
      "Epoch: 3\n",
      "0.051356698757444974\n",
      "0.062031775356444996\n",
      "Epoch: 4\n",
      "0.048580605263850885\n",
      "0.05821373660728568\n",
      "Epoch: 5\n",
      "0.0496726306155324\n",
      "0.03331334580980183\n",
      "Epoch: 6\n",
      "0.04454985343727458\n",
      "0.03200822437884199\n",
      "Epoch: 7\n",
      "0.04223497866587422\n",
      "0.047061608060175786\n",
      "Epoch: 8\n",
      "0.04328598488791613\n",
      "0.06899170894394047\n",
      "Epoch: 9\n",
      "0.04437115823839122\n",
      "0.04713463097505155\n",
      "Epoch: 10\n",
      "0.024933386907832755\n",
      "0.02225807583454298\n",
      "Epoch: 11\n",
      "0.02496764376428473\n",
      "0.02436130740443332\n",
      "Epoch: 12\n",
      "0.025478769763140008\n",
      "0.02428522583159065\n",
      "Epoch: 13\n",
      "0.023232396098137542\n",
      "0.022378310238309496\n",
      "Epoch: 14\n",
      "0.022554284013494907\n",
      "0.01841486067041842\n",
      "Epoch: 15\n",
      "0.021593289019619988\n",
      "0.017128392006270587\n",
      "Epoch: 16\n",
      "0.022323106251860736\n",
      "0.023237230489939975\n",
      "Epoch: 17\n",
      "0.02404987590671226\n",
      "0.024999140872751013\n",
      "Epoch: 18\n",
      "0.022300254841866263\n",
      "0.022538518980582012\n",
      "Epoch: 19\n",
      "0.023480018028749328\n",
      "0.02991446067881043\n",
      "Epoch: 20\n",
      "0.015473554702566616\n",
      "0.013088202538256155\n",
      "Epoch: 21\n",
      "0.014417909547319141\n",
      "0.014021076205608551\n",
      "Epoch: 22\n",
      "0.015167069947892742\n",
      "0.015578554588046245\n",
      "Epoch: 23\n",
      "0.014829313295649627\n",
      "0.025666306017228635\n",
      "Epoch: 24\n",
      "0.014994317923083145\n",
      "0.014634564143761963\n",
      "Epoch: 25\n",
      "0.015678157318234298\n",
      "0.0188710026031913\n",
      "Epoch: 26\n",
      "0.015093328110651782\n",
      "0.013989741658861021\n",
      "Epoch: 27\n",
      "0.014827506505298516\n",
      "0.01401719036130089\n",
      "Epoch: 28\n",
      "0.014536675295858004\n",
      "0.013954370640931302\n",
      "Epoch: 29\n",
      "0.015325455422043888\n",
      "0.021599856000648288\n",
      "Epoch: 30\n",
      "0.01119119676286573\n",
      "0.011021647399047652\n",
      "Epoch: 31\n",
      "0.01131037215645847\n",
      "0.011504150650125666\n",
      "Epoch: 32\n",
      "0.01085729855185491\n",
      "0.010474834592969273\n",
      "Epoch: 33\n",
      "0.0112851516378214\n",
      "0.01161594171435354\n",
      "Epoch: 34\n",
      "0.010860131551453378\n",
      "0.009897491328956676\n",
      "Epoch: 35\n",
      "0.010717755158111686\n",
      "0.009962706006263033\n",
      "Epoch: 36\n",
      "0.010793582693167991\n",
      "0.010580454969385755\n",
      "Epoch: 37\n",
      "0.01081750237790402\n",
      "0.00976988675120083\n",
      "Epoch: 38\n",
      "0.01061287714583159\n",
      "0.011182727114828594\n",
      "Epoch: 39\n",
      "0.01091942635957821\n",
      "0.010918600367403997\n",
      "Epoch: 40\n",
      "0.00889633096585385\n",
      "0.009389979528350523\n",
      "Epoch: 41\n",
      "0.008828685876324016\n",
      "0.008739934502045799\n",
      "Epoch: 42\n",
      "0.008804418134786829\n",
      "0.009288428842410212\n",
      "Epoch: 43\n",
      "0.008615306200226769\n",
      "0.008363370389815827\n",
      "Epoch: 44\n",
      "0.008798938150448521\n",
      "0.008182730767657631\n",
      "Epoch: 45\n",
      "0.00853199782932279\n",
      "0.009434827853965544\n",
      "Epoch: 46\n",
      "0.008577467088798585\n",
      "0.00845462463576041\n",
      "Epoch: 47\n",
      "0.008432426544004556\n",
      "0.008578531410876167\n",
      "Epoch: 48\n",
      "0.008584443214658677\n",
      "0.009443214470138628\n",
      "Epoch: 49\n",
      "0.00848014891062121\n",
      "0.008355224410024675\n",
      "6 0.008355224410024675\n",
      "Run: 7\n",
      "0.9968117141979747\n",
      "Epoch: 0\n",
      "0.17278409692517016\n",
      "0.11964772294595605\n",
      "Epoch: 1\n",
      "0.08735379657446174\n",
      "0.0567659031603398\n",
      "Epoch: 2\n",
      "0.06387607521355676\n",
      "0.05839868236762413\n",
      "Epoch: 3\n",
      "0.05461483107683307\n",
      "0.053677859001254546\n",
      "Epoch: 4\n",
      "0.05043450120501802\n",
      "0.06923974978417391\n",
      "Epoch: 5\n",
      "0.04599403611609887\n",
      "0.037714264399255626\n",
      "Epoch: 6\n",
      "0.04878167086826579\n",
      "0.04454964985779952\n",
      "Epoch: 7\n",
      "0.045462106998456875\n",
      "0.03559386555025412\n",
      "Epoch: 8\n",
      "0.04145668202363595\n",
      "0.03960864511645923\n",
      "Epoch: 9\n",
      "0.04064344214475568\n",
      "0.02938808250655711\n",
      "Epoch: 10\n",
      "0.023365350015410513\n",
      "0.02461148664679058\n",
      "Epoch: 11\n",
      "0.02368111719351873\n",
      "0.025211072326783324\n",
      "Epoch: 12\n",
      "0.024591719597083284\n",
      "0.026090734206263733\n",
      "Epoch: 13\n",
      "0.02407200583093072\n",
      "0.019836545379803283\n",
      "Epoch: 14\n",
      "0.023571526624436956\n",
      "0.02917594358586939\n",
      "Epoch: 15\n",
      "0.023345899986452423\n",
      "0.020429959207831416\n",
      "Epoch: 16\n",
      "0.023772445932991104\n",
      "0.026769200630951673\n",
      "Epoch: 17\n",
      "0.02444603071671736\n",
      "0.02069020392082166\n",
      "Epoch: 18\n",
      "0.02286996438397182\n",
      "0.027471592558867997\n",
      "Epoch: 19\n",
      "0.02394801001628366\n",
      "0.01879890846339549\n",
      "Epoch: 20\n",
      "0.015453965766482725\n",
      "0.013820043837313278\n",
      "Epoch: 21\n",
      "0.01600481897321515\n",
      "0.014825013106019469\n",
      "Epoch: 22\n",
      "0.014988339358751546\n",
      "0.013918327236751793\n",
      "Epoch: 23\n",
      "0.015419808452406869\n",
      "0.013188879087465466\n",
      "Epoch: 24\n",
      "0.01438845549000689\n",
      "0.015014329648693092\n",
      "Epoch: 25\n",
      "0.014437582311529695\n",
      "0.013752133060734195\n",
      "Epoch: 26\n",
      "0.01437039850588917\n",
      "0.013453575353651104\n",
      "Epoch: 27\n",
      "0.014877608249662444\n",
      "0.014798883727962675\n",
      "Epoch: 28\n",
      "0.014669333480469504\n",
      "0.013860273824775504\n",
      "Epoch: 29\n",
      "0.015523621463671589\n",
      "0.012301426334488497\n",
      "Epoch: 30\n",
      "0.010798787756357342\n",
      "0.01275570483176125\n",
      "Epoch: 31\n",
      "0.011210079088414204\n",
      "0.011225925798044045\n",
      "Epoch: 32\n",
      "0.010700743952384073\n",
      "0.009587251802713581\n",
      "Epoch: 33\n",
      "0.010848738554159354\n",
      "0.011252434403559164\n",
      "Epoch: 34\n",
      "0.011044879980545375\n",
      "0.011563476304218057\n",
      "Epoch: 35\n",
      "0.010703241610826808\n",
      "0.011219340405205003\n",
      "Epoch: 36\n",
      "0.010918217261405516\n",
      "0.01077216302337547\n",
      "Epoch: 37\n",
      "0.010813363227498485\n",
      "0.010986490124651027\n",
      "Epoch: 38\n",
      "0.010470761995293287\n",
      "0.009480033932504739\n",
      "Epoch: 39\n",
      "0.010252206833683886\n",
      "0.009171371446427656\n",
      "Epoch: 40\n",
      "0.008595526453973434\n",
      "0.008859159052462928\n",
      "Epoch: 41\n",
      "0.008678887260430201\n",
      "0.009149355333192943\n",
      "Epoch: 42\n",
      "0.008604298288446444\n",
      "0.00820679903108612\n",
      "Epoch: 43\n",
      "0.00861566284811488\n",
      "0.00878337625090353\n",
      "Epoch: 44\n",
      "0.008455871440219198\n",
      "0.008828336544411286\n",
      "Epoch: 45\n",
      "0.008561237610592798\n",
      "0.008875092878497526\n",
      "Epoch: 46\n",
      "0.008447664782579523\n",
      "0.008274226419416664\n",
      "Epoch: 47\n",
      "0.008299233062643907\n",
      "0.009073536312826036\n",
      "Epoch: 48\n",
      "0.008501055242049915\n",
      "0.008040242637889605\n",
      "Epoch: 49\n",
      "0.008364181072920474\n",
      "0.008155853921380185\n",
      "7 0.008155853921380185\n",
      "Run: 8\n",
      "1.0228091954486445\n",
      "Epoch: 0\n",
      "0.16263690244886675\n",
      "0.06877628661459312\n",
      "Epoch: 1\n",
      "0.08594738858664641\n",
      "0.09122337364169653\n",
      "Epoch: 2\n",
      "0.07126893103668408\n",
      "0.04924513233709149\n",
      "Epoch: 3\n",
      "0.05522239588935918\n",
      "0.05299576033030462\n",
      "Epoch: 4\n",
      "0.054587236169027165\n",
      "0.03734738655657566\n",
      "Epoch: 5\n",
      "0.04957815156740253\n",
      "0.05464533504709834\n",
      "Epoch: 6\n",
      "0.042930015277306666\n",
      "0.045773582734909724\n",
      "Epoch: 7\n",
      "0.04819385197970405\n",
      "0.047143546526058344\n",
      "Epoch: 8\n",
      "0.0403185041686811\n",
      "0.04278330337911029\n",
      "Epoch: 9\n",
      "0.04011695777171553\n",
      "0.04047996843110013\n",
      "Epoch: 10\n",
      "0.02317703644894209\n",
      "0.018286233868821\n",
      "Epoch: 11\n",
      "0.02438197039464285\n",
      "0.02169496652459202\n",
      "Epoch: 12\n",
      "0.024600419102171145\n",
      "0.02588544476657262\n",
      "Epoch: 13\n",
      "0.02280012938172149\n",
      "0.017954070880477957\n",
      "Epoch: 14\n",
      "0.02310149238928716\n",
      "0.022620398004619346\n",
      "Epoch: 15\n",
      "0.023502554505284934\n",
      "0.02251031685136695\n",
      "Epoch: 16\n",
      "0.024348946751160838\n",
      "0.020097747445106506\n",
      "Epoch: 17\n",
      "0.023905193882455933\n",
      "0.029114962051608018\n",
      "Epoch: 18\n",
      "0.021834610378391517\n",
      "0.019947976754338015\n",
      "Epoch: 19\n",
      "0.023623523587957607\n",
      "0.02228390366417443\n",
      "Epoch: 20\n",
      "0.015549978998024017\n",
      "0.01665818008223141\n",
      "Epoch: 21\n",
      "0.015931593194181914\n",
      "0.016970491419669997\n",
      "Epoch: 22\n",
      "0.015417740990869788\n",
      "0.01857232681959431\n",
      "Epoch: 23\n",
      "0.01546785722121058\n",
      "0.01440190262201213\n",
      "Epoch: 24\n",
      "0.014439530707932136\n",
      "0.013125839675467432\n",
      "Epoch: 25\n",
      "0.014875939210014621\n",
      "0.018426911553433456\n",
      "Epoch: 26\n",
      "0.015969603679423017\n",
      "0.013603695879282895\n",
      "Epoch: 27\n",
      "0.014815902403825021\n",
      "0.014974265571254364\n",
      "Epoch: 28\n",
      "0.01458697819725785\n",
      "0.015214926324915723\n",
      "Epoch: 29\n",
      "0.014126747686077579\n",
      "0.017648865048613516\n",
      "Epoch: 30\n",
      "0.011279374354216998\n",
      "0.010766966491246421\n",
      "Epoch: 31\n",
      "0.011106510947229253\n",
      "0.010437509084113117\n",
      "Epoch: 32\n",
      "0.011198726550446736\n",
      "0.010440785720675194\n",
      "Epoch: 33\n",
      "0.010981268685554824\n",
      "0.009864397417004511\n",
      "Epoch: 34\n",
      "0.011270959011199011\n",
      "0.010785625726839498\n",
      "Epoch: 35\n",
      "0.010808545602230879\n",
      "0.012125572375680349\n",
      "Epoch: 36\n",
      "0.010956408653783\n",
      "0.010931292011264304\n",
      "Epoch: 37\n",
      "0.010552771846505493\n",
      "0.009523137274754845\n",
      "Epoch: 38\n",
      "0.010727955186212057\n",
      "0.012199531608530378\n",
      "Epoch: 39\n",
      "0.010772337760954542\n",
      "0.009824016610309627\n",
      "Epoch: 40\n",
      "0.008894881110336428\n",
      "0.008651856256619794\n",
      "Epoch: 41\n",
      "0.008908988525035966\n",
      "0.008490156093102996\n",
      "Epoch: 42\n",
      "0.008949263870363211\n",
      "0.008882252703187987\n",
      "Epoch: 43\n",
      "0.008783708829469106\n",
      "0.008493033912600367\n",
      "Epoch: 44\n",
      "0.008802666291558126\n",
      "0.00970255539050413\n",
      "Epoch: 45\n",
      "0.008727513791654928\n",
      "0.009517835257156548\n",
      "Epoch: 46\n",
      "0.0087090896877271\n",
      "0.008755488176575454\n",
      "Epoch: 47\n",
      "0.00862087805489864\n",
      "0.008603358501659386\n",
      "Epoch: 48\n",
      "0.008491528797549108\n",
      "0.00849395336354064\n",
      "Epoch: 49\n",
      "0.008710686758149677\n",
      "0.008888570963335951\n",
      "8 0.008888570963335951\n",
      "Run: 9\n",
      "1.0012916279956698\n",
      "Epoch: 0\n",
      "0.16845318723790115\n",
      "0.1214865709116566\n",
      "Epoch: 1\n",
      "0.08511835254103062\n",
      "0.06888608262306661\n",
      "Epoch: 2\n",
      "0.06325693466169469\n",
      "0.06310273894087004\n",
      "Epoch: 3\n",
      "0.05490028640269884\n",
      "0.04421168801127351\n",
      "Epoch: 4\n",
      "0.050024060472424026\n",
      "0.058429174809134565\n",
      "Epoch: 5\n",
      "0.04519956518015533\n",
      "0.029998624300787924\n",
      "Epoch: 6\n",
      "0.04506642667547567\n",
      "0.04360795493084879\n",
      "Epoch: 7\n",
      "0.04304077684173535\n",
      "0.06703430607740302\n",
      "Epoch: 8\n",
      "0.04632824753389286\n",
      "0.04424302058941976\n",
      "Epoch: 9\n",
      "0.04192811797292961\n",
      "0.05077722175519739\n",
      "Epoch: 10\n",
      "0.023582320064633677\n",
      "0.024233359657046094\n",
      "Epoch: 11\n",
      "0.024274653820612002\n",
      "0.022509825059387367\n",
      "Epoch: 12\n",
      "0.02541586491770431\n",
      "0.020703066602436593\n",
      "Epoch: 13\n",
      "0.023841843898480874\n",
      "0.02357499890240433\n",
      "Epoch: 14\n",
      "0.022377806044460158\n",
      "0.02821815173047071\n",
      "Epoch: 15\n",
      "0.022595706477659405\n",
      "0.019999610266495438\n",
      "Epoch: 16\n",
      "0.02547910420162225\n",
      "0.03415138864329492\n",
      "Epoch: 17\n",
      "0.023191374647467455\n",
      "0.022059126317799382\n",
      "Epoch: 18\n",
      "0.02217473636392242\n",
      "0.022117051248642383\n",
      "Epoch: 19\n",
      "0.023895455975434743\n",
      "0.022459244131823652\n",
      "Epoch: 20\n",
      "0.015953833722960553\n",
      "0.013609071926566685\n",
      "Epoch: 21\n",
      "0.015044425973883335\n",
      "0.014759058629351784\n",
      "Epoch: 22\n",
      "0.015603836247464642\n",
      "0.020505850395238667\n",
      "Epoch: 23\n",
      "0.014853498653337738\n",
      "0.013630783278586023\n",
      "Epoch: 24\n",
      "0.014894797056967946\n",
      "0.013661466404755629\n",
      "Epoch: 25\n",
      "0.0152544528573344\n",
      "0.013869732048078731\n",
      "Epoch: 26\n",
      "0.01582421714465454\n",
      "0.013463095702263672\n",
      "Epoch: 27\n",
      "0.014731112555182335\n",
      "0.014902878737302672\n",
      "Epoch: 28\n",
      "0.014677143578410323\n",
      "0.012658366830237355\n",
      "Epoch: 29\n",
      "0.014956310698835296\n",
      "0.012426621593021991\n",
      "Epoch: 30\n",
      "0.010924943598638492\n",
      "0.010571867938779178\n",
      "Epoch: 31\n",
      "0.01085477723609074\n",
      "0.010540321346979908\n",
      "Epoch: 32\n",
      "0.010826908737271879\n",
      "0.010453055806465272\n",
      "Epoch: 33\n",
      "0.01052127929551716\n",
      "0.009719623540604516\n",
      "Epoch: 34\n",
      "0.010426049955640337\n",
      "0.009458242728214827\n",
      "Epoch: 35\n",
      "0.010675556021851662\n",
      "0.010423412320960779\n",
      "Epoch: 36\n",
      "0.010806615416186105\n",
      "0.010729112660555984\n",
      "Epoch: 37\n",
      "0.010378259730259742\n",
      "0.010171239128794696\n",
      "Epoch: 38\n",
      "0.010372494598868798\n",
      "0.009252069982721878\n",
      "Epoch: 39\n",
      "0.010362152992001938\n",
      "0.010310499058959977\n",
      "Epoch: 40\n",
      "0.008559252381928673\n",
      "0.008311110803788324\n",
      "Epoch: 41\n",
      "0.008547462400656514\n",
      "0.00823383171155001\n",
      "Epoch: 42\n",
      "0.00859582595194297\n",
      "0.009057062917690928\n",
      "Epoch: 43\n",
      "0.008449646792087151\n",
      "0.00935607981909925\n",
      "Epoch: 44\n",
      "0.008514889809248416\n",
      "0.009637705601107882\n",
      "Epoch: 45\n",
      "0.008407778040691483\n",
      "0.008844839812354621\n",
      "Epoch: 46\n",
      "0.008393837592393538\n",
      "0.008187784440906398\n",
      "Epoch: 47\n",
      "0.008350970132596558\n",
      "0.007993247299509676\n",
      "Epoch: 48\n",
      "0.008290487555768777\n",
      "0.009485528752520622\n",
      "Epoch: 49\n",
      "0.008239946446792601\n",
      "0.00786752606563823\n",
      "9 0.00786752606563823\n",
      "0 [1.0012916279956698, 0.1214865709116566, 0.06888608262306661, 0.06310273894087004, 0.04421168801127351, 0.058429174809134565, 0.029998624300787924, 0.04360795493084879, 0.06703430607740302, 0.04424302058941976, 0.05077722175519739, 0.024233359657046094, 0.022509825059387367, 0.020703066602436593, 0.02357499890240433, 0.02821815173047071, 0.019999610266495438, 0.03415138864329492, 0.022059126317799382, 0.022117051248642383, 0.022459244131823652, 0.013609071926566685, 0.014759058629351784, 0.020505850395238667, 0.013630783278586023, 0.013661466404755629, 0.013869732048078731, 0.013463095702263672, 0.014902878737302672, 0.012658366830237355, 0.012426621593021991, 0.010571867938779178, 0.010540321346979908, 0.010453055806465272, 0.009719623540604516, 0.009458242728214827, 0.010423412320960779, 0.010729112660555984, 0.010171239128794696, 0.009252069982721878, 0.010310499058959977, 0.008311110803788324, 0.00823383171155001, 0.009057062917690928, 0.00935607981909925, 0.009637705601107882, 0.008844839812354621, 0.008187784440906398, 0.007993247299509676, 0.009485528752520622, 0.00786752606563823]\n",
      "Pad: 5\n",
      "Run: 0\n",
      "0.9954329190659337\n",
      "Epoch: 0\n",
      "0.1443410618121561\n",
      "0.08931258221491589\n",
      "Epoch: 1\n",
      "0.08471727519645356\n",
      "0.0710050776215212\n",
      "Epoch: 2\n",
      "0.06675801505480194\n",
      "0.06595607306189777\n",
      "Epoch: 3\n",
      "0.05365094904664147\n",
      "0.05665780633717077\n",
      "Epoch: 4\n",
      "0.0507510830211686\n",
      "0.04061645589717955\n",
      "Epoch: 5\n",
      "0.04862086256252951\n",
      "0.04708327583830396\n",
      "Epoch: 6\n",
      "0.04656114540739509\n",
      "0.055585991405678215\n",
      "Epoch: 7\n",
      "0.04156191734728054\n",
      "0.03570370925444877\n",
      "Epoch: 8\n",
      "0.041199598773346224\n",
      "0.03673637287465681\n",
      "Epoch: 9\n",
      "0.04456809052044264\n",
      "0.029751531612419058\n",
      "Epoch: 10\n",
      "0.025228076538041933\n",
      "0.025374164042659686\n",
      "Epoch: 11\n",
      "0.02750270164142421\n",
      "0.025372371392222703\n",
      "Epoch: 12\n",
      "0.025602878906283877\n",
      "0.02084734067102545\n",
      "Epoch: 13\n",
      "0.024861072974999843\n",
      "0.026670272345654666\n",
      "Epoch: 14\n",
      "0.02376482788258727\n",
      "0.02721877320254862\n",
      "Epoch: 15\n",
      "0.024132654457389435\n",
      "0.018458537070728198\n",
      "Epoch: 16\n",
      "0.0228323457758961\n",
      "0.024256944358057808\n",
      "Epoch: 17\n",
      "0.022429331062994606\n",
      "0.027211393302422948\n",
      "Epoch: 18\n",
      "0.023043671886625816\n",
      "0.023940968181705102\n",
      "Epoch: 19\n",
      "0.02263587617107987\n",
      "0.02282534523146751\n",
      "Epoch: 20\n",
      "0.01590114608188742\n",
      "0.013383173083184374\n",
      "Epoch: 21\n",
      "0.01599415013242833\n",
      "0.015081272050338157\n",
      "Epoch: 22\n",
      "0.016113977148052072\n",
      "0.018619672337990778\n",
      "Epoch: 23\n",
      "0.01611677160963154\n",
      "0.018750698652183928\n",
      "Epoch: 24\n",
      "0.016250845145805215\n",
      "0.0184534291665841\n",
      "Epoch: 25\n",
      "0.016665669325448107\n",
      "0.016659432331834978\n",
      "Epoch: 26\n",
      "0.015664633680898987\n",
      "0.018610815493047994\n",
      "Epoch: 27\n",
      "0.016079943563454435\n",
      "0.015593001732668199\n",
      "Epoch: 28\n",
      "0.01652296660358843\n",
      "0.016212433342843724\n",
      "Epoch: 29\n",
      "0.01576111277245218\n",
      "0.015832813543056545\n",
      "Epoch: 30\n",
      "0.011904460528512573\n",
      "0.011802905507011019\n",
      "Epoch: 31\n",
      "0.011851401978674403\n",
      "0.012036213317060174\n",
      "Epoch: 32\n",
      "0.011958917845731776\n",
      "0.011705441450430953\n",
      "Epoch: 33\n",
      "0.011694463918956899\n",
      "0.013053000055151642\n",
      "Epoch: 34\n",
      "0.011878973167313234\n",
      "0.01374449724698934\n",
      "Epoch: 35\n",
      "0.011965989841883129\n",
      "0.010556838913998945\n",
      "Epoch: 36\n",
      "0.011562520946426957\n",
      "0.011594476626669348\n",
      "Epoch: 37\n",
      "0.011630216406956606\n",
      "0.012918294504743244\n",
      "Epoch: 38\n",
      "0.01164992978601731\n",
      "0.010467672115737514\n",
      "Epoch: 39\n",
      "0.011328855749070499\n",
      "0.013409635899733985\n",
      "Epoch: 40\n",
      "0.009745225852384465\n",
      "0.009779148229881685\n",
      "Epoch: 41\n",
      "0.009730042637784209\n",
      "0.00958056120953188\n",
      "Epoch: 42\n",
      "0.009715538853924954\n",
      "0.009741495769048925\n",
      "Epoch: 43\n",
      "0.009672419424077816\n",
      "0.009226690430296003\n",
      "Epoch: 44\n",
      "0.009591178594746452\n",
      "0.009162631492472428\n",
      "Epoch: 45\n",
      "0.009486701298556\n",
      "0.009540153560010367\n",
      "Epoch: 46\n",
      "0.009448911651816161\n",
      "0.009827107558521675\n",
      "Epoch: 47\n",
      "0.009483793737217638\n",
      "0.010387366008671961\n",
      "Epoch: 48\n",
      "0.00939697452895416\n",
      "0.010131745040780515\n",
      "Epoch: 49\n",
      "0.009384045849856193\n",
      "0.009584018300301977\n",
      "0 0.009584018300301977\n",
      "Run: 1\n",
      "1.0066809502895921\n",
      "Epoch: 0\n",
      "0.16161340014150483\n",
      "0.082167895809107\n",
      "Epoch: 1\n",
      "0.08193481833586702\n",
      "0.06295727839460596\n",
      "Epoch: 2\n",
      "0.07052268364350311\n",
      "0.06382048694104014\n",
      "Epoch: 3\n",
      "0.05953998839686392\n",
      "0.05863316387512896\n",
      "Epoch: 4\n",
      "0.051798912729282165\n",
      "0.03495004577416694\n",
      "Epoch: 5\n",
      "0.04896422277124657\n",
      "0.04618879206827842\n",
      "Epoch: 6\n",
      "0.04725312594564457\n",
      "0.04015591618917824\n",
      "Epoch: 7\n",
      "0.046679996339662466\n",
      "0.057634603341284674\n",
      "Epoch: 8\n",
      "0.0426270550742629\n",
      "0.041782469634199515\n",
      "Epoch: 9\n",
      "0.04146768324790173\n",
      "0.04190189956352697\n",
      "Epoch: 10\n",
      "0.026960299047459557\n",
      "0.020326061980085797\n",
      "Epoch: 11\n",
      "0.025737663609106676\n",
      "0.028550443859785446\n",
      "Epoch: 12\n",
      "0.024231279768173408\n",
      "0.029094903207806055\n",
      "Epoch: 13\n",
      "0.025904179081408074\n",
      "0.02184527307872486\n",
      "Epoch: 14\n",
      "0.026862864927352348\n",
      "0.024211656253100955\n",
      "Epoch: 15\n",
      "0.024342599745978077\n",
      "0.023706951490567008\n",
      "Epoch: 16\n",
      "0.02349498364583269\n",
      "0.020702523251202365\n",
      "Epoch: 17\n",
      "0.023796622393092548\n",
      "0.02327164224243461\n",
      "Epoch: 18\n",
      "0.02378530621444952\n",
      "0.02537733602002845\n",
      "Epoch: 19\n",
      "0.02356158671045705\n",
      "0.03226097039987508\n",
      "Epoch: 20\n",
      "0.016408572832460777\n",
      "0.015034398240459268\n",
      "Epoch: 21\n",
      "0.016602353269263403\n",
      "0.017334521464363206\n",
      "Epoch: 22\n",
      "0.016159988555500604\n",
      "0.015570606828077871\n",
      "Epoch: 23\n",
      "0.01724725230724289\n",
      "0.01566569169608556\n",
      "Epoch: 24\n",
      "0.016426323887117178\n",
      "0.01775551396258379\n",
      "Epoch: 25\n",
      "0.016834907865813875\n",
      "0.0197533815562565\n",
      "Epoch: 26\n",
      "0.016251873211331258\n",
      "0.026625618447724264\n",
      "Epoch: 27\n",
      "0.01665002683785133\n",
      "0.014067718451769906\n",
      "Epoch: 28\n",
      "0.01669760447794033\n",
      "0.018470469770363707\n",
      "Epoch: 29\n",
      "0.016163706101906428\n",
      "0.016254644433956855\n",
      "Epoch: 30\n",
      "0.012113546298678557\n",
      "0.01322130307153202\n",
      "Epoch: 31\n",
      "0.011906469513633056\n",
      "0.014200778786289447\n",
      "Epoch: 32\n",
      "0.011851590154492442\n",
      "0.011037262601803377\n",
      "Epoch: 33\n",
      "0.011905490960998577\n",
      "0.01116298054876097\n",
      "Epoch: 34\n",
      "0.0115908548536936\n",
      "0.012044196444094268\n",
      "Epoch: 35\n",
      "0.011692485085404769\n",
      "0.012266222218386247\n",
      "Epoch: 36\n",
      "0.011647208969407075\n",
      "0.010805767687998014\n",
      "Epoch: 37\n",
      "0.011771887724989938\n",
      "0.011084445323376713\n",
      "Epoch: 38\n",
      "0.011703264966854476\n",
      "0.011667572452097374\n",
      "Epoch: 39\n",
      "0.011794675952387479\n",
      "0.0111749599018367\n",
      "Epoch: 40\n",
      "0.009652135828673636\n",
      "0.010625248127325904\n",
      "Epoch: 41\n",
      "0.009849807421687728\n",
      "0.009603921427242312\n",
      "Epoch: 42\n",
      "0.009551316723900527\n",
      "0.0096098239996536\n",
      "Epoch: 43\n",
      "0.00960630870440582\n",
      "0.010416144169994368\n",
      "Epoch: 44\n",
      "0.009523331535547186\n",
      "0.009189055374008603\n",
      "Epoch: 45\n",
      "0.009514746271634067\n",
      "0.009183211787785694\n",
      "Epoch: 46\n",
      "0.00953422607972243\n",
      "0.010112717580796016\n",
      "Epoch: 47\n",
      "0.00954895918357579\n",
      "0.009315684042576322\n",
      "Epoch: 48\n",
      "0.009415094625637721\n",
      "0.010864345700611011\n",
      "Epoch: 49\n",
      "0.009462929509936657\n",
      "0.009137752070273564\n",
      "1 0.009137752070273564\n",
      "Run: 2\n",
      "0.985472483036574\n",
      "Epoch: 0\n",
      "0.14972670440693037\n",
      "0.11687516071106074\n",
      "Epoch: 1\n",
      "0.07588581928212079\n",
      "0.0902135990872921\n",
      "Epoch: 2\n",
      "0.06661589615578123\n",
      "0.05595328446543135\n",
      "Epoch: 3\n",
      "0.05716855314312852\n",
      "0.06432635660894448\n",
      "Epoch: 4\n",
      "0.0523423915328749\n",
      "0.03771439030060719\n",
      "Epoch: 5\n",
      "0.04795648208892089\n",
      "0.04459252029846539\n",
      "Epoch: 6\n",
      "0.05118041243258631\n",
      "0.045912942643553833\n",
      "Epoch: 7\n",
      "0.05017008511822496\n",
      "0.049309100870232214\n",
      "Epoch: 8\n",
      "0.04557083151121333\n",
      "0.04370711318370013\n",
      "Epoch: 9\n",
      "0.042961473798641236\n",
      "0.06435465570757515\n",
      "Epoch: 10\n",
      "0.02756521340688778\n",
      "0.023814156529624597\n",
      "Epoch: 11\n",
      "0.02640502905978792\n",
      "0.037026566576969344\n",
      "Epoch: 12\n",
      "0.025104620698584768\n",
      "0.024196982003559242\n",
      "Epoch: 13\n",
      "0.025894817128573777\n",
      "0.030362453115230892\n",
      "Epoch: 14\n",
      "0.02390311783165089\n",
      "0.03257627943821717\n",
      "Epoch: 15\n",
      "0.023897977587694186\n",
      "0.028896779464957945\n",
      "Epoch: 16\n",
      "0.02591621291685442\n",
      "0.026271468111190188\n",
      "Epoch: 17\n",
      "0.02269631909803138\n",
      "0.03330970722890925\n",
      "Epoch: 18\n",
      "0.025429862620512722\n",
      "0.021428117648611078\n",
      "Epoch: 19\n",
      "0.02547761769255885\n",
      "0.03154296773209353\n",
      "Epoch: 20\n",
      "0.017161599679184292\n",
      "0.02031927550706314\n",
      "Epoch: 21\n",
      "0.01613481171443709\n",
      "0.01992952692853578\n",
      "Epoch: 22\n",
      "0.016483729096307798\n",
      "0.013362318090912595\n",
      "Epoch: 23\n",
      "0.01650156882669762\n",
      "0.014931338992937526\n",
      "Epoch: 24\n",
      "0.016347568821402092\n",
      "0.014602379520511022\n",
      "Epoch: 25\n",
      "0.01520625712828405\n",
      "0.016863940534676658\n",
      "Epoch: 26\n",
      "0.01559886595396165\n",
      "0.013847138153323613\n",
      "Epoch: 27\n",
      "0.015639273235592555\n",
      "0.013166794952212513\n",
      "Epoch: 28\n",
      "0.015578336709950236\n",
      "0.01481948653145082\n",
      "Epoch: 29\n",
      "0.015932297120798466\n",
      "0.013878177825517923\n",
      "Epoch: 30\n",
      "0.011698168144448573\n",
      "0.012621768711142067\n",
      "Epoch: 31\n",
      "0.011700652410127077\n",
      "0.011129789468668605\n",
      "Epoch: 32\n",
      "0.012034863964800024\n",
      "0.012766778186687588\n",
      "Epoch: 33\n",
      "0.012033239486299863\n",
      "0.010649966322944238\n",
      "Epoch: 34\n",
      "0.011774235078064521\n",
      "0.012535152419332007\n",
      "Epoch: 35\n",
      "0.011436702991886705\n",
      "0.011937001003843761\n",
      "Epoch: 36\n",
      "0.011739920288619032\n",
      "0.010902294996867568\n",
      "Epoch: 37\n",
      "0.011732523923001281\n",
      "0.010386976212430454\n",
      "Epoch: 38\n",
      "0.01153175390436445\n",
      "0.010666195202247764\n",
      "Epoch: 39\n",
      "0.011309873323170905\n",
      "0.010698455548663333\n",
      "Epoch: 40\n",
      "0.00941690059244138\n",
      "0.009546481424877129\n",
      "Epoch: 41\n",
      "0.009497809854565276\n",
      "0.010022760928677599\n",
      "Epoch: 42\n",
      "0.00946048673176847\n",
      "0.010306343294814724\n",
      "Epoch: 43\n",
      "0.009515688750070694\n",
      "0.009421500981716235\n",
      "Epoch: 44\n",
      "0.009437501784304914\n",
      "0.009115145976011263\n",
      "Epoch: 45\n",
      "0.009573680485573277\n",
      "0.009747461882398056\n",
      "Epoch: 46\n",
      "0.00929947326585534\n",
      "0.009395769415732502\n",
      "Epoch: 47\n",
      "0.009394526338383002\n",
      "0.009139154620243062\n",
      "Epoch: 48\n",
      "0.009256753946829122\n",
      "0.009806851620396628\n",
      "Epoch: 49\n",
      "0.009406320347807196\n",
      "0.009792430648303707\n",
      "2 0.009792430648303707\n",
      "Run: 3\n",
      "1.009370967745781\n",
      "Epoch: 0\n",
      "0.15078996855663718\n",
      "0.11285146461887052\n",
      "Epoch: 1\n",
      "0.07739785161720647\n",
      "0.062446501666272525\n",
      "Epoch: 2\n",
      "0.06498400053169462\n",
      "0.05160747735135374\n",
      "Epoch: 3\n",
      "0.054104859476865386\n",
      "0.14200006658575148\n",
      "Epoch: 4\n",
      "0.05963529215841845\n",
      "0.06978866875579115\n",
      "Epoch: 5\n",
      "0.055099417575547704\n",
      "0.05543781836604467\n",
      "Epoch: 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-e23bc97dbc6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mtrain_loss_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-f398a29908c9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer, scheduler, l2_loss, epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# remove padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-ad2a29ab68cb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, sub)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-1e9f5bed0541>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, sub)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_x\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msize_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# x2 = F.interpolate(x2, size=size_list[1], mode='bilinear')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-5a5f7ee109fb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, size)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mbatchsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#Compute Fourier coeffcients up to factor of e^(- something constant)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfftn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Multiply relevant Fourier modes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "final_error = []\n",
    "\n",
    "train_loss_total = []\n",
    "test_loss_total = []\n",
    "\n",
    "for pad in range(0, max_pad, 5):\n",
    "    print(\"Pad: \" + str(pad))\n",
    "    \n",
    "    train_loss_pad = []\n",
    "    test_loss_pad = []\n",
    "    \n",
    "    train_loader, test_loader = pad_data(x_train, x_test, pad)\n",
    "    \n",
    "    for run in range(10):\n",
    "        print(\"Run: \" + str(run))\n",
    "        \n",
    "        train_loss_epoch = []\n",
    "        test_loss_epoch = []\n",
    "        \n",
    "        model = Net2d(modes, width).cuda()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = 1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = step_size, gamma = gamma)\n",
    "\n",
    "        l2_loss = LpLoss(size_average=True)\n",
    "        \n",
    "        test_loss_initial = test(test_loader, model, l2_loss)\n",
    "        test_loss_epoch.append(test_loss_initial)\n",
    "        \n",
    "        print(test_loss_initial)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(\"Epoch: \"+str(epoch))\n",
    "            \n",
    "            train_loss = train(train_loader, model, optimizer, scheduler, l2_loss, epoch)\n",
    "            train_loss_epoch.append(train_loss)\n",
    "            \n",
    "            print(train_loss)\n",
    "            \n",
    "            test_loss = test(test_loader, model, l2_loss)\n",
    "            test_loss_epoch.append(test_loss)\n",
    "            \n",
    "            print(test_loss)\n",
    "\n",
    "        train_loss_pad.append(train_loss_epoch)\n",
    "        test_loss_pad.append(test_loss_epoch)\n",
    "        \n",
    "        print(run, test_loss_epoch[-1])\n",
    "    \n",
    "    train_loss_total.append(train_loss_pad)\n",
    "    test_loss_total.append(test_loss_pad)\n",
    "    \n",
    "    print(pad, test_loss_pad[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d396a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
